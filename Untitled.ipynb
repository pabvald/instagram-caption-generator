{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ae1d87-b88f-4c0d-8db7-4c838fea971e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation metrics for Image Captioning \n",
    "\n",
    "This notebook introduces 3 of the most widespread evaluation metrics for Image Captioning: BLEU, METEOR and  ROUGE-L. It also includes examples of usage of the Python implementations these three metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0822d-d7f6-4af1-a647-d2cff9d52dc0",
   "metadata": {},
   "source": [
    "## 1. BLEU (Bilingual Evaluation Understudy)\n",
    " \n",
    "\n",
    "The [BLEU (Bilingual Evaluation Understudy)](https://www.aclweb.org/anthology/P02-1040.pdf) was originally thought as a metric to evaluate the quality of machine-translated text; however, it is actually the main metric in image captioning tasks. \n",
    "\n",
    "Given a a candidate sentence $c$ and a set of reference sentences $R =\\{r_1, r_2, ..., r_m\\}$, the *BLEU* metric for n-grams of size $n$, $w_n$, is:\n",
    "\n",
    "\n",
    "$$BLEU_n(c, R) = \\frac{\\sum_{w_n \\in c} min(Count_t(w_n), max_{j = \\{1, ..., |R|\\}} Count_{r_j}(w_n))}{\\sum_{w_n \\in t} Count_t(w_n)}$$\n",
    "\n",
    "The $BLEU_n$ score will be in the range $[0,1]$, and the closer to 1 it is, the more similar the candidate sentence $c$ is to the reference sentences $R$.\n",
    "\n",
    "The $BLEU_n$ score is usually calculated for $n=1,2,3,4$ and reported in a single $BLEU$ score combinening the $BLEU_1$, $BLEU_2$, $BLEU_3$ and $BLEU_4$ scores in a weighted average: \n",
    "\n",
    "$$BLEU(c, R) = \\sum_{n=1}^{N=4} \\alpha_n BLEU_n(c, R)$$\n",
    "\n",
    "being the uniform weights $\\alpha_n = \\frac{1}{4}$ the ones that are usually\n",
    "\n",
    "### Examples \n",
    "\n",
    "The nltk library provides an [implementation](https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.bleu_score) of the *BLEU* score that is very easy to use. In this case, we will consider that the set of reference sentences contains a single sentence, $R = \\{r\\}$, and we will use the [sentence_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu) function; however, another [corpus_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.corpus_bleu) function is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aaf4c083-4469-429d-bd29-d0c27bdf15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu as bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63fb0f-e2c5-4252-a351-71810fcf54fa",
   "metadata": {},
   "source": [
    "Our reference and candidate sentences will be  the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5f3c44d7-26bc-45e2-8ecf-7f30823440eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'fox', 'jumped', 'over', 'the', 'lazy',  'dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e328bd-fb0d-4c08-917e-eb4c96201e40",
   "metadata": {},
   "source": [
    "We compute the individual $BLEU_n$ scores, for $n = 1,2,3,4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbe1c656-fbb1-4667-b18e-d42f669ec4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_1 =  0.772184789761521\n",
      "BLEU_2 =  0.6303549304175682\n",
      "BLEU_3 =  0.5883312683897303\n",
      "BLEU_4 =  0.5294981415507573\n"
     ]
    }
   ],
   "source": [
    "bleu_1 = bleu_score(references, candidate, weights=(1, 0, 0, 0))\n",
    "bleu_2 = bleu_score(references, candidate, weights=(0, 1, 0, 0))\n",
    "bleu_3 = bleu_score(references, candidate, weights=(0, 0, 1, 0))\n",
    "bleu_4 = bleu_score(references, candidate, weights=(0, 0, 0, 1))\n",
    "\n",
    "print(\"BLEU_1 = \", bleu_1)\n",
    "print(\"BLEU_2 = \", bleu_2)\n",
    "print(\"BLEU_3 = \", bleu_3)\n",
    "print(\"BLEU_4 = \", bleu_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e595590-f6d2-422e-8dc6-17b26165f1b7",
   "metadata": {},
   "source": [
    "We can then compute the combine $BLEU$ score by combining the $BLEU_n$ scores or directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1d701716-9175-46f9-9015-f372eb767d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU =  0.6240195441936914\n"
     ]
    }
   ],
   "source": [
    "bleu_combined = (1/4)*bleu_1 + (1/4)*bleu_2 + (1/4)*bleu_3 + (1/4)*bleu_4\n",
    "\n",
    "bleu = bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))\n",
    "\n",
    "assert abs(bleu_combined - bleu) < 1e-2, 'incorrect calculation'\n",
    "\n",
    "print(\"BLEU = \", bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99c81d-2798-4d8a-9cf8-fea99c05f1e4",
   "metadata": {},
   "source": [
    "If the sentences are identical, the *BLEU* score will be 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "273778d4-7cac-46da-adf6-604c6b47aec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf87d4e-c9be-4dbb-b56f-d225628b2a16",
   "metadata": {},
   "source": [
    "Changing a single word will already decrease it by nearly 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "961131e7-a1e6-4b39-930e-e1f53c8aa1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7506238537503395"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127b18c-714f-4c04-b79d-8ba0a7db80c9",
   "metadata": {},
   "source": [
    "Simply removing the two adjectives for the *fox* and the *dog* will decrease it down to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0eae390-94af-40d9-bf46-c0515280dc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5025431541540227"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'brown', 'fox', 'jumped', 'over', 'the', 'dog']\n",
    "bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2325be-1a98-4ae0-bd85-9924e70d1a22",
   "metadata": {},
   "source": [
    "## 2. METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "\n",
    "The [METEOR (Metric for Evaluation of Translation with Explicit ORdering)](https://www.researchgate.net/publication/228346240_METEOR_An_automatic_metric_for_MT_evaluation_with_high_levels_of_correlation_with_human_judgments) score is based on the harmonic mean of unigram precision and recall and it includes several features that aren't found in other metrics, such as *stemming* and *synonymy matching*. This metric seeks correlation at the sentence level, whereas *BLEU* seeks correlation at the corpus level.\n",
    "\n",
    "In order to compute the METEOR score, it is necessary to compute the precision, $P$, and the recall, $R$, of the unigrams. Let $c$ be the candidate sentence and $r$ be the reference sentence:\n",
    "\n",
    "$$P = \\frac{m}{\\sum_{w \\in c} 1}$$\n",
    "\n",
    "$$R = \\frac{m}{\\sum_{w \\in r} 1}$$\n",
    "\n",
    "where $m$ is the number of unigrams in the candidate sentence that are also found in the reference sentence.\n",
    "\n",
    "Precision and recall are then combined using the harmonic mean in the following way:\n",
    "\n",
    "$$F_{mean} = \\frac{10 \\cdot P \\cdot R}{R + 9P}$$\n",
    "\n",
    "In order to account for the congruity of larger n-grams, longer n-gram matches are used to compute a penalty $p$ for the alignment. The more mappings there are that are not adjacent in the reference and the candidate sentence, the higher the penalty will be.\n",
    "\n",
    "In order to compute this penalty, unigrams are grouped into the fewest possible chunks, where a chunk is defined as a set of unigrams that are adjacent in the candidate and in the reference. The longer the adjacent mappings between the candidate and the reference, the fewer chunks there are. A translation that is identical to the reference will give just one chunk. The penalty p is computed as follows:\n",
    "\n",
    "$$p = 0.5 (\\frac{k}{m})^3$$\n",
    "\n",
    "where $k$ is the number of chunks and $m$ is the number of unigrams that have been map.\n",
    "\n",
    "The final score is calculated as \n",
    "\n",
    "$$M = F_{mean}(1-p)$$\n",
    "\n",
    "The penalty $p$ will reduce the $F_{mean}$ up to 50% if there are no bigram or longer matches.\n",
    "\n",
    "### Examples\n",
    "\n",
    "The Python nltk library also provides an [implementation](https://www.nltk.org/_modules/nltk/translate/meteor_score.html) for the *METEOR* score. Since the *METEOR* score makes use of *Wordnet*, it is necessary to download the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d898ee0d-7968-4085-9efa-6e9a3b72a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pabvald/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c89095ad-3408-4d6a-8293-1d7667d62486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score as meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd36e9-8059-400c-9588-d466c12ae111",
   "metadata": {},
   "source": [
    "We must notice the two main differences from the *bleu_score* implementation: \n",
    "\n",
    "1. The sentences are provided as a string, not as a list of tokens.\n",
    "2. The order of the parameters is inverted, in this case the *candidate* sentence is provided before the *reference* sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "eb7e92d5-fa66-4ebd-bc5b-74b03ff930ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993141289437586"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"the quick brown fox jumped over the lazy dog\"\n",
    "candidate = \"the quick brown fox jumped over the lazy dog\" \n",
    "meteor_score(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad0eda-387f-4e75-a130-dea69f5eb8d0",
   "metadata": {},
   "source": [
    "Changing the order of the words will obviously decrease the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1c9c895-4320-4ec8-8126-c6421fa9875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647462277091907"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"the quick brown fox jumped over the lazy dog\"\n",
    "candidate = \"the brown quick fox jumped over lazy the dog\" \n",
    "meteor_score(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29009533-e5d8-407c-a8e5-048d637beaed",
   "metadata": {},
   "source": [
    "Since *METEOR* uses stemming and synonymy matching, the following changes do not decrese the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4120b9e9-4022-4b26-8762-3c6e583192b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993141289437586"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"the quick brown fox jumped over the lazy dog\"\n",
    "candidate = \"the fast brown fox jumps over the lazy dogs\" \n",
    "meteor_score(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec88186-591b-45dd-a9fb-7e547a46ec40",
   "metadata": {},
   "source": [
    "## 3. ROUGE-L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37c52762-a973-4e8a-bf11-6490720e1b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "scores = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')\n",
    "\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d664ff2c-9d94-4433-89ad-3a74470ce7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.75, recall=0.6666666666666666, fmeasure=0.7058823529411765),\n",
       " 'rougeL': Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69242c71-43cb-4904-83ff-0d6f3312d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858c6fa-71a0-4918-93b0-dfe5aac70207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlcv_project",
   "language": "python",
   "name": "hlcv_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
