{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36ae1d87-b88f-4c0d-8db7-4c838fea971e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation metrics for Image Captioning tasks\n",
    "\n",
    "**Denis Kokorev, Deepanshu Metha, and Pablo Valdunciel**\n",
    "\n",
    "deko00002@stud.uni-saarland.de, 7007817  <br>\n",
    "deme00001@stud.uni-saarland.de, 7011083 <br>\n",
    "pava00001@stud.uni-saarland.de, 7010186 <br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "This notebook introduces 4 of the most widespread evaluation metrics for Image Captioning tasks: \n",
    "\n",
    "1. BLEU \n",
    "2. METEOR\n",
    "3. ROUGE-L\n",
    "4. CIDEr \n",
    "\n",
    "It also includes examples of usage of some Python implementations of  these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0822d-d7f6-4af1-a647-d2cff9d52dc0",
   "metadata": {},
   "source": [
    "## 1. BLEU (Bilingual Evaluation Understudy)\n",
    " \n",
    "\n",
    "The [BLEU (Bilingual Evaluation Understudy)](https://www.aclweb.org/anthology/P02-1040.pdf) was originally thought as a metric to evaluate the quality of machine-translated text; however, it is actually the main metric in image captioning tasks. \n",
    "\n",
    "Given a a candidate sentence $c$ and a set of reference sentences $R =\\{r_1, r_2, ..., r_m\\}$, the *BLEU* metric for n-grams of size $n$, $w_n$, is:\n",
    "\n",
    "\n",
    "$$BLEU_n(c, R) = \\frac{\\sum_{w_n \\in c} min(Count_t(w_n), max_{j = \\{1, ..., |R|\\}} Count_{r_j}(w_n))}{\\sum_{w_n \\in c} Count_c(w_n)}$$\n",
    "\n",
    "The $BLEU_n$ score will be in the range $[0,1]$, and the closer to 1 it is, the more similar the candidate sentence $c$ is to the reference sentences $R$.\n",
    "\n",
    "The $BLEU_n$ score is usually calculated for $n=1,2,3,4$ and reported in a single $BLEU$ score combining the $BLEU_1$, $BLEU_2$, $BLEU_3$ and $BLEU_4$ scores in a weighted average: \n",
    "\n",
    "$$BLEU(c, R) = \\sum_{n=1}^{N=4} \\alpha_n BLEU_n(c, R)$$\n",
    "\n",
    "being the uniform weights $\\alpha_n = \\frac{1}{4}$ the ones that are normally used.\n",
    "\n",
    "#### Examples \n",
    "\n",
    "The nltk library provides an [implementation](https://www.nltk.org/api/nltk.translate.html#module-nltk.translate.bleu_score) of the *BLEU* score that is very easy to use. In this case, we will consider that the set of reference sentences contains a single sentence, $R = \\{r\\}$, and we will use the [sentence_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.sentence_bleu) function; however, another [corpus_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.corpus_bleu) function is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaf4c083-4469-429d-bd29-d0c27bdf15c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu as bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63fb0f-e2c5-4252-a351-71810fcf54fa",
   "metadata": {},
   "source": [
    "Our reference and candidate sentences will be  the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3c44d7-26bc-45e2-8ecf-7f30823440eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'fox', 'jumped', 'over', 'the', 'lazy',  'dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e328bd-fb0d-4c08-917e-eb4c96201e40",
   "metadata": {},
   "source": [
    "We compute the individual $BLEU_n$ scores, for $n = 1,2,3,4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbe1c656-fbb1-4667-b18e-d42f669ec4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU_1 =  0.772184789761521\n",
      "BLEU_2 =  0.6303549304175682\n",
      "BLEU_3 =  0.5883312683897303\n",
      "BLEU_4 =  0.5294981415507573\n"
     ]
    }
   ],
   "source": [
    "bleu_1 = bleu_score(references, candidate, weights=(1, 0, 0, 0))\n",
    "bleu_2 = bleu_score(references, candidate, weights=(0, 1, 0, 0))\n",
    "bleu_3 = bleu_score(references, candidate, weights=(0, 0, 1, 0))\n",
    "bleu_4 = bleu_score(references, candidate, weights=(0, 0, 0, 1))\n",
    "\n",
    "print(\"BLEU_1 = \", bleu_1)\n",
    "print(\"BLEU_2 = \", bleu_2)\n",
    "print(\"BLEU_3 = \", bleu_3)\n",
    "print(\"BLEU_4 = \", bleu_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e595590-f6d2-422e-8dc6-17b26165f1b7",
   "metadata": {},
   "source": [
    "We can then compute the combine $BLEU$ score by combining the $BLEU_n$ scores or directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d701716-9175-46f9-9015-f372eb767d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU =  0.6240195441936914\n"
     ]
    }
   ],
   "source": [
    "bleu_combined = (1/4)* (bleu_1 + bleu_2 + bleu_3 + bleu_4)\n",
    "\n",
    "bleu = bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))\n",
    "\n",
    "assert abs(bleu_combined - bleu) < 1e-2, 'incorrect calculation'\n",
    "\n",
    "print(\"BLEU = \", bleu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99c81d-2798-4d8a-9cf8-fea99c05f1e4",
   "metadata": {},
   "source": [
    "If the sentences are identical, the *BLEU* score will be 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "273778d4-7cac-46da-adf6-604c6b47aec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf87d4e-c9be-4dbb-b56f-d225628b2a16",
   "metadata": {},
   "source": [
    "Changing a single word will already decrease it by nearly 0.25, even if the word is a synonym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "961131e7-a1e6-4b39-930e-e1f53c8aa1dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7506238537503395"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'fast', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n",
    "bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127b18c-714f-4c04-b79d-8ba0a7db80c9",
   "metadata": {},
   "source": [
    "Simply removing the two adjectives for the *fox* and the *dog* will decrease it down to 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0eae390-94af-40d9-bf46-c0515280dc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5025431541540227"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'brown', 'fox', 'jumped', 'over', 'the', 'dog']\n",
    "bleu_score(references, candidate, weights=(1/4, 1/4, 1/4, 1/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2325be-1a98-4ae0-bd85-9924e70d1a22",
   "metadata": {},
   "source": [
    "## 2. METEOR (Metric for Evaluation of Translation with Explicit ORdering)\n",
    "\n",
    "The [METEOR (Metric for Evaluation of Translation with Explicit ORdering)](https://www.researchgate.net/publication/228346240_METEOR_An_automatic_metric_for_MT_evaluation_with_high_levels_of_correlation_with_human_judgments) score is based on the harmonic mean of unigram precision and recall and it includes several features that aren't found in other metrics, such as **stemming** and **synonymy matching**. This metric seeks correlation at the sentence level, whereas *BLEU* seeks correlation at the corpus level.\n",
    "\n",
    "In order to compute the METEOR score, it is necessary to compute the precision, $P$, and the recall, $R$, of the unigrams. Let $c$ be the candidate sentence and $r$ be the reference sentence:\n",
    "\n",
    "$$P = \\frac{m}{\\sum_{w \\in c} 1} = \\frac{m}{|c|}$$\n",
    "\n",
    "$$R = \\frac{m}{\\sum_{w \\in r} 1}= \\frac{m}{|r|}$$\n",
    "\n",
    "where $m$ is the number of unigrams in the candidate sentence that are also found in the reference sentence.\n",
    "\n",
    "Precision and recall are then combined using the harmonic mean in the following way:\n",
    "\n",
    "$$F_{mean} = \\frac{10 \\cdot P \\cdot R}{R + 9P}$$\n",
    "\n",
    "In order to account for the congruity of larger n-grams, longer n-gram matches are used to compute a penalty $p$ for the alignment. The more mappings there are that are not adjacent in the reference and the candidate sentence, the higher the penalty will be.\n",
    "\n",
    "In order to compute this penalty, unigrams are grouped into the fewest possible chunks, where a chunk is defined as a set of unigrams that are adjacent in the candidate and in the reference. The longer the adjacent mappings between the candidate and the reference, the fewer chunks there are. A translation that is identical to the reference will give just one chunk. The penalty p is computed as follows:\n",
    "\n",
    "$$p = 0.5 (\\frac{k}{m})^3$$\n",
    "\n",
    "where $k$ is the number of chunks and $m$ is the number of unigrams that have been mapped.\n",
    "\n",
    "The final score is calculated as \n",
    "\n",
    "$$M = F_{mean}(1-p)$$\n",
    "\n",
    "The penalty $p$ will reduce the $F_{mean}$ up to 50% if there are no bigram or longer matches.\n",
    "\n",
    "#### Examples\n",
    "\n",
    "The Python nltk library also provides an [implementation](https://www.nltk.org/_modules/nltk/translate/meteor_score.html) for the *METEOR* score. Since the *METEOR* score makes use of *Wordnet*, it is necessary to download the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d898ee0d-7968-4085-9efa-6e9a3b72a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/pabvald/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c89095ad-3408-4d6a-8293-1d7667d62486",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score as meteor_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd36e9-8059-400c-9588-d466c12ae111",
   "metadata": {},
   "source": [
    "We must notice the two main differences from the *bleu_score* implementation: \n",
    "\n",
    "1. The sentences are provided as a string, not as a list of tokens.\n",
    "2. The order of the parameters is inverted, in this case the *candidate* sentence is provided before the *reference* sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb7e92d5-fa66-4ebd-bc5b-74b03ff930ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993141289437586"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"the quick brown fox jumped over the lazy dog\"\n",
    "candidate = \"the quick brown fox jumped over the lazy dog\" \n",
    "meteor_score(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfad0eda-387f-4e75-a130-dea69f5eb8d0",
   "metadata": {},
   "source": [
    "Changing the order of the words will obviously decrease the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1c9c895-4320-4ec8-8126-c6421fa9875d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7647462277091907"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"the quick brown fox jumped over the lazy dog\"\n",
    "candidate = \"the brown quick fox jumped over lazy the dog\" \n",
    "meteor_score(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29009533-e5d8-407c-a8e5-048d637beaed",
   "metadata": {},
   "source": [
    "Since *METEOR* uses stemming and synonymy matching, the following changes do not decrese the score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4120b9e9-4022-4b26-8762-3c6e583192b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993141289437586"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference = \"the quick brown fox jumped over the lazy dog\"\n",
    "candidate = \"the fast brown fox jumps over the lazy dogs\" \n",
    "meteor_score(candidate, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec88186-591b-45dd-a9fb-7e547a46ec40",
   "metadata": {},
   "source": [
    "## 3. ROUGE-L\n",
    "\n",
    "[ROUGE (Recall-Oriented Understudy for\n",
    "Gisting Evaluation)](https://www.aclweb.org/anthology/W04-1013.pdf) includes measures to automatically determine the quality of a summary by\n",
    "comparing it to other (ideal) summaries created by\n",
    "humans. The measures count the number of overlapping units such as n-gram, word sequences, and\n",
    "word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. The measure that is used to measure the results in image captioning task is **ROUGE-L: Longest Common Subsequence**.\n",
    "\n",
    "Let $c$ be the candidate sentence and $r$ be the reference sentence. The *ROUGE-L score* will be: \n",
    "\n",
    "\n",
    "\n",
    "$$ ROUGE_L(c, r) = 2 \\cdot \\frac{1}{\\frac{1}{P} + \\frac{1}{R}}$$ \n",
    "\n",
    "where: \n",
    "* $ P = \\frac{LCS(c,r)}{|c|}$ is the precision\n",
    "\n",
    "* $ R = \\frac{LCS(c,r)}{|r|}$ is the recall \n",
    "\n",
    "* $LCS(c,r)$ is the longest common subsequence between sentences $c$ and $r\n",
    "\n",
    "\n",
    "The $ROUGE_L$ score will be in the range $[0,1]$, and the closer to 1 it is, the more similar the candidate sentence $c$ is to the reference sentence $r$\n",
    "\n",
    "#### Examples:\n",
    "\n",
    "The Python [rouge_score](https://github.com/google-research/google-research/tree/master/rouge) package allows to compute the $ROUGE_L$ score  as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37c52762-a973-4e8a-bf11-6490720e1b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "score = scorer.score('The quick brown fox jumps over the lazy dog',\n",
    "                      'The quick brown dog jumps on the log.')['rougeL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d664ff2c-9d94-4433-89ad-3a74470ce7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score(precision=0.625, recall=0.5555555555555556, fmeasure=0.5882352941176471)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51f556b8-34cf-49a3-94af-cd30ee3b92c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5882352941176471"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_L = score.fmeasure\n",
    "rouge_L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199af7f7-3d33-42c2-b5a1-52d8f6b4df61",
   "metadata": {},
   "source": [
    "We can see that the package provides with the final $ROUGE_L$ score (*fmeasure*) as well as the *precision* and *recall* used to compute it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a3577-2d3b-4bf2-83e7-219723ae8f65",
   "metadata": {},
   "source": [
    "## 4. CIDEr \n",
    "\n",
    "[CIDEr (Consensus-based Image Description Evaluation)](https://arxiv.org/abs/1411.5726) was specifically design to evaluate image-captioning taskt. It gives more weightage to important n-grams and has a higher correlation with human consensus scores compared to metrics.\n",
    "\n",
    "Let $c$ be the candidate sentence and $R =\\{r_1, r_2, ..., r_k\\}$ be a set of reference sentences. The *CIDEr score* considering n-grams of size $n$ is:\n",
    "\n",
    "\n",
    "$$CIDEr_n(c, R) = \\frac{1}{|R|} \\sum_{j=1}^{k} \\frac{g^n(c) \\cdot g^n(r_j)}{||g^n(c)|| \\cdot ||g^n(r_j)||}$$\n",
    "\n",
    "where \n",
    "\n",
    "* $g^n(x)$ rerprsents the [TF-IDF](https://es.wikipedia.org/wiki/Tf-idf) scores of all n-grams in $x$\n",
    "\n",
    "The scores from n-grams of varying lengths can be combined as follows: \n",
    "\n",
    "$$CIDEr(c, R) = \\sum_{n=1}^{N} w_n \\cdot CIDEr_n(c, R) $$\n",
    "\n",
    "Empirically, it has been shown that uniform weights, $w_n = \\frac{1}{n}$ work the best.\n",
    "\n",
    "#### Examples \n",
    "\n",
    "There is not an implementation of *CIDEr* alone, but the [Microsoft COCO Caption Evaluation code]() implements several metrics including *BLEU* (1,2,3 and 4), *METEOR*, *ROUGE-L* and **CIDEr**. This code is included in the package [evaluation](./evaluation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5bb899a-9dc8-47bc-87ce-e92850b85f59",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'bleu': [0.579197412311677, 0.40424019147824525, 0.2784669955736699, 0.19079338759125988], 'cider': 0.6028573119212788, 'meteor': 0.19525467177780284, 'rouge': 0.39625269357570847}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import sys\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from evaluation.eval import eval\n",
    "\n",
    "\n",
    "gts_path = '../evaluation/examples/gts.json' # ground-truths\n",
    "res_path = '../evaluation/examples/res.json' # candidates\n",
    "\n",
    "with open(gts_path, 'r') as f: \n",
    "    gts = json.load(f)\n",
    "with open(res_path, 'r') as f:\n",
    "    res = json.load(f)\n",
    "\n",
    "mp = eval(gts, res, metrics=['bleu', 'cider', 'meteor', 'rouge'])\n",
    "print(mp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('instagram-caption-generator-Z18861q1': pipenv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "f58a678dbd1670e0124423fbd42524fb68cfc59d5c077e2c6f76e74bd8aab77e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}